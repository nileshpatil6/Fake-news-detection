# -*- coding: utf-8 -*-
"""fakenewsdetect.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1dZ3DdvYzwYqqEdRFRzMew1jjpydfSm6L
"""

# <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
# Step 0: Install necessary libraries
# <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
!pip install -q transformers datasets pandas scikit-learn kaggle torch

# <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
# Step 1: Setup Kaggle API for dataset download
# <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
import os
from google.colab import files

print("Please upload your kaggle.json file")
uploaded = files.upload()

if 'kaggle.json' in uploaded:
    print("kaggle.json uploaded successfully!")
    # Make directory and move kaggle.json
    !mkdir -p ~/.kaggle
    !mv kaggle.json ~/.kaggle/
    !chmod 600 ~/.kaggle/kaggle.json
else:
    print("kaggle.json not found. Please upload it to use the Kaggle API.")
    # You might want to stop execution here or provide an alternative
    raise Exception("Kaggle API key not provided. Cannot download dataset.")

# <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
# Step 2: Download and Prepare the Dataset
# <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
import pandas as pd
from sklearn.model_selection import train_test_split

# Download the dataset (Fake and real news dataset by ClÃ©ment Bisaillon)
# https://www.kaggle.com/datasets/clmentbisaillon/fake-and-real-news-dataset
print("\nDownloading dataset...")
!kaggle datasets download -d clmentbisaillon/fake-and-real-news-dataset -p ./ --unzip

# Load the datasets
try:
    df_true = pd.read_csv("True.csv")
    df_fake = pd.read_csv("Fake.csv")
    print("Datasets loaded successfully.")
except FileNotFoundError:
    print("Error: CSV files not found. Make sure the dataset downloaded correctly.")
    print("Files in current directory:", os.listdir("./"))
    raise

# Add labels: 1 for Real, 0 for Fake
df_true['label'] = 1
df_fake['label'] = 0

# Combine title and text into a single feature, as text content is richer
# If 'title' or 'text' can be NaN, fill them with empty strings
df_true['full_text'] = df_true['title'].fillna('') + " " + df_true['text'].fillna('')
df_fake['full_text'] = df_fake['title'].fillna('') + " " + df_fake['text'].fillna('')

# Select relevant columns and combine datasets
df_true = df_true[['full_text', 'label']]
df_fake = df_fake[['full_text', 'label']]

df_combined = pd.concat([df_true, df_fake], ignore_index=True)

# Shuffle the dataset
df_combined = df_combined.sample(frac=1, random_state=42).reset_index(drop=True)

# Handle potential empty strings after concatenation if both title and text were empty
df_combined = df_combined[df_combined['full_text'].str.strip().astype(bool)]
df_combined.dropna(subset=['full_text'], inplace=True)


print(f"\nCombined dataset shape: {df_combined.shape}")
print("Dataset head:\n", df_combined.head())
print("\nLabel distribution:\n", df_combined['label'].value_counts())

# For faster demonstration, let's use a subset of the data.
# Remove this or increase sample_size for full training.
SAMPLE_SIZE = 10000 # Using 10k samples for quicker run. Full dataset is ~45k
if len(df_combined) > SAMPLE_SIZE:
    print(f"\nSubsampling dataset to {SAMPLE_SIZE} for faster training...")
    df_combined = df_combined.sample(n=SAMPLE_SIZE, random_state=42).reset_index(drop=True)
    print(f"New dataset shape: {df_combined.shape}")
    print("New label distribution:\n", df_combined['label'].value_counts())


# Split into training and validation sets
train_texts, val_texts, train_labels, val_labels = train_test_split(
    df_combined['full_text'].tolist(),
    df_combined['label'].tolist(),
    test_size=0.2,
    random_state=42,
    stratify=df_combined['label'].tolist() # Ensure balanced splits
)

print(f"\nTraining samples: {len(train_texts)}")
print(f"Validation samples: {len(val_texts)}")

# <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
# Step 3: Load BERT Tokenizer and Model
# <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
import torch
# !!!!! MODIFIED IMPORT SECTION !!!!!
from transformers import BertTokenizer, BertForSequenceClassification, get_linear_schedule_with_warmup
from torch.optim import AdamW # <--- MOVED AdamW import here
# !!!!! END OF MODIFIED IMPORT SECTION !!!!!
from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler
from sklearn.metrics import classification_report, accuracy_score

# Check for GPU
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"\nUsing device: {device}")

# Load BERT tokenizer
MODEL_NAME = 'bert-base-uncased'
tokenizer = BertTokenizer.from_pretrained(MODEL_NAME)

# Tokenize the texts
MAX_LENGTH = 256 # Choose based on your text length distribution and VRAM

def tokenize_data(texts, labels):
    input_ids = []
    attention_masks = []

    for text in texts:
        encoded_dict = tokenizer.encode_plus(
                            text,
                            add_special_tokens=True, # Add '[CLS]' and '[SEP]'
                            max_length=MAX_LENGTH,
                            padding='max_length',   # Pad/truncate to max_length
                            truncation=True,
                            return_attention_mask=True,
                            return_tensors='pt',    # Return PyTorch tensors
                       )
        input_ids.append(encoded_dict['input_ids'])
        attention_masks.append(encoded_dict['attention_mask'])

    input_ids = torch.cat(input_ids, dim=0)
    attention_masks = torch.cat(attention_masks, dim=0)
    labels = torch.tensor(labels)
    return input_ids, attention_masks, labels

print("\nTokenizing training data...")
train_input_ids, train_attention_masks, train_labels_tensor = tokenize_data(train_texts, train_labels)
print("Tokenizing validation data...")
val_input_ids, val_attention_masks, val_labels_tensor = tokenize_data(val_texts, val_labels)

# Create TensorDatasets
train_dataset = TensorDataset(train_input_ids, train_attention_masks, train_labels_tensor)
val_dataset = TensorDataset(val_input_ids, val_attention_masks, val_labels_tensor)

# Create DataLoaders
BATCH_SIZE = 16 # Adjust based on GPU memory. 8 or 16 is a good start for BERT-base.

train_dataloader = DataLoader(
            train_dataset,
            sampler=RandomSampler(train_dataset), # Select batches randomly
            batch_size=BATCH_SIZE
        )

validation_dataloader = DataLoader(
            val_dataset,
            sampler=SequentialSampler(val_dataset), # Pull out batches sequentially
            batch_size=BATCH_SIZE
        )

print(f"\nDataLoaders created with batch size: {BATCH_SIZE}")

# Load BERT model for sequence classification
model = BertForSequenceClassification.from_pretrained(
    MODEL_NAME,
    num_labels=2, # Binary classification: Real (1) or Fake (0)
    output_attentions=False,
    output_hidden_states=False,
)
model.to(device) # Move model to GPU if available
print(f"\n{MODEL_NAME} model loaded and moved to {device}")

# <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
# Step 4: Fine-tune BERT
# <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
EPOCHS = 2 # Number of training epochs. Start with 1-3 for BERT fine-tuning.
LEARNING_RATE = 2e-5 # AdamW's default is 1e-3, but 2e-5 to 5e-5 is common for BERT.
EPSILON = 1e-8 # Adam's epsilon for numerical stability

optimizer = AdamW(model.parameters(), lr=LEARNING_RATE, eps=EPSILON) # This line is fine as AdamW is now correctly imported

# Total number of training steps is [number of batches] x [number of epochs].
total_steps = len(train_dataloader) * EPOCHS

# Create the learning rate scheduler.
scheduler = get_linear_schedule_with_warmup(optimizer,
                                            num_warmup_steps=0, # Default value in run_glue.py
                                            num_training_steps=total_steps)

import time
import datetime

def format_time(elapsed):
    elapsed_rounded = int(round((elapsed)))
    return str(datetime.timedelta(seconds=elapsed_rounded))

print(f"\nStarting training for {EPOCHS} epochs...")

for epoch_i in range(0, EPOCHS):
    print("")
    print(f'======== Epoch {epoch_i + 1} / {EPOCHS} ========')
    print('Training...')
    t0 = time.time()
    total_train_loss = 0
    model.train() # Put model in training mode

    for step, batch in enumerate(train_dataloader):
        if step % 40 == 0 and not step == 0: # Progress update every 40 batches
            elapsed = format_time(time.time() - t0)
            print(f'  Batch {step:>5,}  of  {len(train_dataloader):>5,}.    Elapsed: {elapsed}.')

        # Unpack batch and move to device
        b_input_ids = batch[0].to(device)
        b_input_mask = batch[1].to(device)
        b_labels = batch[2].to(device)

        model.zero_grad() # Clear previously calculated gradients

        # Forward pass
        outputs = model(b_input_ids,
                        token_type_ids=None, # Not used by BERT for sequence classification here
                        attention_mask=b_input_mask,
                        labels=b_labels)

        loss = outputs.loss
        total_train_loss += loss.item()

        loss.backward() # Perform backward pass
        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0) # Clip gradients
        optimizer.step() # Update parameters
        scheduler.step() # Update learning rate

    avg_train_loss = total_train_loss / len(train_dataloader)
    training_time = format_time(time.time() - t0)
    print("")
    print(f"  Average training loss: {avg_train_loss:.2f}")
    print(f"  Training epoch took: {training_time}")

    # <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
    # Step 5: Evaluate the Model (on validation set after each epoch)
    # <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
    print("")
    print("Running Validation...")
    t0 = time.time()
    model.eval() # Put model in evaluation mode

    total_eval_accuracy = 0
    total_eval_loss = 0
    all_preds = []
    all_true_labels = []

    for batch in validation_dataloader:
        b_input_ids = batch[0].to(device)
        b_input_mask = batch[1].to(device)
        b_labels = batch[2].to(device)

        with torch.no_grad(): # Tell PyTorch not to compute gradients
            outputs = model(b_input_ids,
                            token_type_ids=None,
                            attention_mask=b_input_mask,
                            labels=b_labels)

        loss = outputs.loss
        logits = outputs.logits

        total_eval_loss += loss.item()

        # Move logits and labels to CPU for sklearn
        logits = logits.detach().cpu().numpy()
        label_ids = b_labels.to('cpu').numpy()

        preds = torch.argmax(torch.tensor(logits), dim=1).flatten()
        all_preds.extend(preds.tolist())
        all_true_labels.extend(label_ids.tolist())

    avg_val_loss = total_eval_loss / len(validation_dataloader)
    validation_time = format_time(time.time() - t0)

    print(f"  Validation Loss: {avg_val_loss:.2f}")
    print(f"  Validation took: {validation_time}")

    # Calculate accuracy and other metrics
    accuracy = accuracy_score(all_true_labels, all_preds)
    print(f"  Accuracy: {accuracy:.4f}")
    print("  Classification Report:")
    # Ensure target_names map correctly: 0 = Fake, 1 = Real
    print(classification_report(all_true_labels, all_preds, target_names=['Fake (0)', 'Real (1)']))

print("\nTraining complete!")

# <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
# Step 6: Save the fine-tuned model (Optional but good practice)
# <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
MODEL_SAVE_PATH = "./fake_news_bert_model/"
if not os.path.exists(MODEL_SAVE_PATH):
    os.makedirs(MODEL_SAVE_PATH)

print(f"\nSaving model to {MODEL_SAVE_PATH}...")
model.save_pretrained(MODEL_SAVE_PATH)
tokenizer.save_pretrained(MODEL_SAVE_PATH)
print("Model and tokenizer saved.")

# To load it later:
# model = BertForSequenceClassification.from_pretrained(MODEL_SAVE_PATH)
# tokenizer = BertTokenizer.from_pretrained(MODEL_SAVE_PATH)
# model.to(device)

# <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
# Step 7: Test with new news headlines/texts
# <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
def predict_news(text, model_to_use, tokenizer_to_use, device_to_use, max_len=MAX_LENGTH):
    model_to_use.eval() # Ensure model is in evaluation mode

    encoded_review = tokenizer_to_use.encode_plus(
        text,
        add_special_tokens=True,
        max_length=max_len,
        return_token_type_ids=False,
        padding='max_length',
        truncation=True,
        return_attention_mask=True,
        return_tensors='pt',
    )

    input_ids = encoded_review['input_ids'].to(device_to_use)
    attention_mask = encoded_review['attention_mask'].to(device_to_use)

    with torch.no_grad():
        outputs = model_to_use(input_ids, attention_mask=attention_mask)
        logits = outputs.logits

    prediction = torch.argmax(logits, dim=1).flatten().item()
    probability = torch.softmax(logits, dim=1).max().item()

    return "Real News" if prediction == 1 else "Fake News", probability

# Test examples
print("\n--- Testing with new examples ---")
test_news_1 = "Scientists discover new planet capable of supporting life in nearby galaxy." # Likely Real
test_news_2 = "BREAKING: Government announces unicorns are real and will be distributed to every household." # Likely Fake
test_news_3 = "Stock market hits all-time high after positive economic reports." # Likely Real
test_news_4 = "Study shows chocolate cures all diseases, doctors baffled." # Likely Fake
test_news_5 = "Local mayor found to be an alien lizard person, confesses at press conference." # Likely Fake

test_samples = [test_news_1, test_news_2, test_news_3, test_news_4, test_news_5]

for i, news_text in enumerate(test_samples):
    prediction, probability = predict_news(news_text, model, tokenizer, device)
    print(f"\nNews ({i+1}): \"{news_text}\"")
    print(f"Prediction: {prediction} (Confidence: {probability*100:.2f}%)")

print("\n--- End of Test ---")

# Optional: To download the saved model from Colab to your local machine
# from google.colab import files
# !zip -r fake_news_bert_model.zip {MODEL_SAVE_PATH}
# files.download('fake_news_bert_model.zip')

# <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
# Step 8: Interactive Testing Loop
# <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

print("\n\n--- Interactive Fake News Detector ---")
print("Enter a news headline or text to check. Type 'quit' or 'exit' to stop.")

# Ensure the model, tokenizer, and device are accessible in this scope
# If you are running this in a new session, you'd need to load them first:
# MODEL_LOAD_PATH = "./fake_news_bert_model/"
# model = BertForSequenceClassification.from_pretrained(MODEL_LOAD_PATH)
# tokenizer = BertTokenizer.from_pretrained(MODEL_LOAD_PATH)
# device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
# model.to(device)
# MAX_LENGTH = 256 # Or whatever you used during training

while True:
    try:
        user_input = input("\nEnter news text: ")
        if user_input.lower() in ['quit', 'exit']:
            print("Exiting interactive detector.")
            break
        if not user_input.strip():
            print("Input is empty. Please try again.")
            continue

        prediction, probability = predict_news(user_input, model, tokenizer, device, max_len=MAX_LENGTH)
        print(f"  -> Prediction: {prediction} (Confidence: {probability*100:.2f}%)")

    except Exception as e:
        print(f"An error occurred: {e}")
        print("Please try again or type 'quit' to exit.")

print("\n--- End of Interactive Session ---")

# <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
# Option 2: Save the model to Google Drive
# <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
from google.colab import drive
import shutil

print("\nMounting Google Drive...")
drive.mount('/content/drive') # This will prompt for authorization

# Define the path in your Google Drive where you want to save the model
# Example: Creates a folder 'MyColabModels/FakeNewsDetector' in your Drive's root
DRIVE_SAVE_PATH_BASE = '/content/drive/MyDrive/MyColabModels/'
DRIVE_MODEL_SAVE_PATH = os.path.join(DRIVE_SAVE_PATH_BASE, 'FakeNewsBERT_Project') # You can name this folder as you like

if not os.path.exists(DRIVE_MODEL_SAVE_PATH):
    os.makedirs(DRIVE_MODEL_SAVE_PATH, exist_ok=True)
    print(f"Created directory in Google Drive: {DRIVE_MODEL_SAVE_PATH}")


target_drive_model_dir = os.path.join(DRIVE_MODEL_SAVE_PATH, os.path.basename(MODEL_SAVE_PATH.strip('/')))

if os.path.exists(target_drive_model_dir):
    print(f"Destination {target_drive_model_dir} already exists. Removing it first.")
    shutil.rmtree(target_drive_model_dir) # Remove if it exists to avoid errors with copytree

print(f"Copying model from '{MODEL_SAVE_PATH}' to '{target_drive_model_dir}' in Google Drive...")
shutil.copytree(MODEL_SAVE_PATH, target_drive_model_dir)

print(f"Model and tokenizer successfully copied to Google Drive at: {target_drive_model_dir}")
print("You can now find it in your Google Drive and generate a shareable link.")

# To get a shareable link:
# 1. Go to your Google Drive (drive.google.com).
# 2. Navigate to 'MyColabModels/FakeNewsBERT_Project' (or whatever path you used).
# 3. Right-click on the 'fake_news_bert_model' folder (or the parent folder like 'FakeNewsBERT_Project').
# 4. Select "Get link" or "Share".
# 5. Change sharing settings to "Anyone with the link can view" (or edit, if required by professionals).
# 6. Copy the link.